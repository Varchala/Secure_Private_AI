{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Split_Learning_varchala_version2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dZbPUdcqNDO-",
        "NArBViQXWpiv",
        "XpPJA5DMuMTF",
        "2LO8a06rv9jv",
        "jJe1PqP7ix7v",
        "8_IC2bRh_9xn",
        "0nc-eXVKAHwO",
        "lqZN-F7xAHwQ",
        "CwEMPtFWAHwS",
        "r4sxEYNmipHR"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPc51FxMthfByN1UHpON5HW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varchala/Secure_Private_AI/blob/main/Split_Learning_varchala_version2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMqthYvLnC7z"
      },
      "source": [
        "!pip install syft==0.2.9 --quiet"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WNtZxwBsnEc",
        "outputId": "523ac2c4-14ff-4f51-fd93-39091105991b"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKe8WQAysjd7"
      },
      "source": [
        "import torch\n",
        "import syft as sy\n",
        "import torch\n",
        "from torchvision import datasets, transforms  # it may raise errors, and you need restart the runtime\n",
        "from torch import nn, optim\n",
        "import syft as sy\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdP9eiGStkbu",
        "outputId": "b27281c0-4775-4a28-9993-16cd95438fd3"
      },
      "source": [
        "import torch\n",
        "import syft as sy\n",
        "\n",
        "# allow pysyft to work its magic on torch tensors\n",
        "hook = sy.TorchHook(torch)\n",
        "\n",
        "# create a virtual worker. in an actual setting this would be on a different machine\n",
        "client = sy.VirtualWorker( hook, id='client' )\n",
        "\n",
        "# define a tensor and send it to the client\n",
        "x = torch.tensor([1,2,3,4,5])\n",
        "# this leaves us with a pointer to the tensor\n",
        "x_pointer = x.send( client )\n",
        "\n",
        "# check out some meta data\n",
        "print( x_pointer )\n",
        "print( client._objects )\n",
        "\n",
        "# we can use this pointers like normal tensors\n",
        "result = x_pointer + x_pointer\n",
        "print( result )\n",
        "\n",
        "# if we want the result we can call get() to send the tensor back to us\n",
        "result_local = result.get()\n",
        "# once we call get() it removes the tensor from the other side and our pointer\n",
        "# becomes invalid\n",
        "print( result_local )\n",
        "print( client._objects )\n",
        "# print( result )"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Wrapper)>[PointerTensor | me:77340015578 -> client:18337107945]\n",
            "{18337107945: tensor([1, 2, 3, 4, 5])}\n",
            "(Wrapper)>[PointerTensor | me:50033037401 -> client:28915619493]\n",
            "tensor([ 2,  4,  6,  8, 10])\n",
            "{18337107945: tensor([1, 2, 3, 4, 5])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpNYXYRl7tSe",
        "outputId": "d44e30e2-cb08-468e-d1c1-d92b1772aa8a"
      },
      "source": [
        "# Data preprocessing\n",
        "hook = sy.TorchHook(torch)\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "trainset = datasets.CIFAR100('cifar100', download=True, train=True, transform=transform)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Torch was already hooked... skipping hooking process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk3AI62y9yi4",
        "outputId": "3f188476-3f07-4ba0-ebcb-c01c70b85584"
      },
      "source": [
        "trainset.data.shape,len(trainset.targets)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 32, 32, 3), 50000)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T22dnQdY-H7A"
      },
      "source": [
        "def extract_n_classes( data, labels,n):\n",
        "    data_cl=[]\n",
        "    y=[]\n",
        "    for i in range(n):\n",
        "            data_cl.append(data[ np.argwhere( labels.reshape(-1) == i ).reshape(-1) ][ : ])\n",
        "            y.extend(np.full((data_cl[i].shape[0]), i, dtype=int))\n",
        "\n",
        "\n",
        "    x = np.vstack( (data_cl) )\n",
        "    y = np.array(y)\n",
        "    return x, y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha6WRbTe-uET"
      },
      "source": [
        "x_train,y_train = extract_n_classes(trainset.data,np.array(trainset.targets),n=100)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmfqu0SvNbk2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHwU7tHu_YKV",
        "outputId": "a1834b72-d69e-4e25-b5e1-6675f6c48548"
      },
      "source": [
        "x_train.shape,y_train.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 32, 32, 3), (50000,))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bkfg2WFs_tsx",
        "outputId": "5feb5dc2-3ad9-40cf-c93e-8d63099d36f8"
      },
      "source": [
        "np.unique(y_train, return_counts=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
              "        68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
              "        85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
              " array([500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
              "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
              "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
              "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
              "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
              "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
              "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
              "        500, 500, 500, 500, 500, 500, 500, 500, 500]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT6DMgPx_2Aa",
        "outputId": "f00e8ba0-fd7a-4b11-f0b3-a3427542f0ec"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  0  0 ... 99 99 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axYEfgN1_cbt"
      },
      "source": [
        "trainset.data = x_train\n",
        "trainset.targets = y_train"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Nm_jjgB_mVr",
        "outputId": "011f7ff8-6ae9-46ed-a249-22f4f1083172"
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False)\n",
        "\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9b37b73e90>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLc3ygiK_nOi",
        "outputId": "0a0c8388-6402-4d5a-e05f-305e6fd9fb62"
      },
      "source": [
        "trainloader.dataset.targets"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0, ..., 99, 99, 99])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoxuyIkPHE4t"
      },
      "source": [
        "class SplitNN:\n",
        "    def __init__(self, models, optimizers):\n",
        "        self.models = models\n",
        "        self.optimizers = optimizers\n",
        "        \n",
        "    def forward(self, x):\n",
        "        a = []\n",
        "        remote_a = []\n",
        "        \n",
        "        a.append(models[0](x))\n",
        "        if a[-1].location == models[1].location:\n",
        "            remote_a.append(a[-1].detach().requires_grad_())\n",
        "        else:\n",
        "            remote_a.append(a[-1].detach().move(models[1].location).requires_grad_())\n",
        "\n",
        "        i=1    \n",
        "        while i < (len(models)-1):\n",
        "            \n",
        "            a.append(models[i](remote_a[-1]))\n",
        "            if a[-1].location == models[i+1].location:\n",
        "                remote_a.append(a[-1].detach().requires_grad_())\n",
        "            else:\n",
        "                remote_a.append(a[-1].detach().move(models[i+1].location).requires_grad_())\n",
        "            \n",
        "            i+=1\n",
        "        \n",
        "        a.append(models[i](remote_a[-1]))\n",
        "        self.a = a\n",
        "        self.remote_a = remote_a\n",
        "        \n",
        "        return a[-1]\n",
        "    \n",
        "    def backward(self):\n",
        "        a=self.a\n",
        "        remote_a=self.remote_a\n",
        "        optimizers = self.optimizers\n",
        "        \n",
        "        i= len(models)-2   \n",
        "        while i > -1:\n",
        "            if remote_a[i].location == a[i].location:\n",
        "                grad_a = remote_a[i].grad.copy()\n",
        "            else:\n",
        "                grad_a = remote_a[i].grad.copy().move(a[i].location)\n",
        "            a[i].backward(grad_a)\n",
        "            i-=1\n",
        "\n",
        "    \n",
        "    def zero_grads(self):\n",
        "        for opt in optimizers:\n",
        "            opt.zero_grad()\n",
        "        \n",
        "    def step(self):\n",
        "        for opt in optimizers:\n",
        "            opt.step()\n",
        "            "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZbPUdcqNDO-"
      },
      "source": [
        "# Define our model segments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOEB8_wdNB-7",
        "outputId": "9c2633eb-a765-4940-febb-0b5cd22000bc"
      },
      "source": [
        "\n",
        "total = 100\n",
        "\n",
        "input_size = 3072\n",
        "hidden_sizes = [128, 640]\n",
        "output_size = 100\n",
        "\n",
        "layers = [nn.Linear(input_size, hidden_sizes[0]), nn.ReLU()]\n",
        "\n",
        "for i in range(1, int(total/2)-1):\n",
        "  if (i%2 != 0):\n",
        "    layers.extend([nn.Linear(hidden_sizes[0], hidden_sizes[1]), nn.ReLU()])\n",
        "  else:\n",
        "    layers.extend([nn.Linear(hidden_sizes[1], hidden_sizes[0]), nn.ReLU()])\n",
        "\n",
        "# the final layer\n",
        "layers.extend([nn.Linear(hidden_sizes[0], output_size), nn.LogSoftmax(dim=1)])\n",
        "\n",
        "print(len(layers))\n",
        "layers"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Linear(in_features=3072, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=100, bias=True),\n",
              " LogSoftmax()]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OOMR6DhxMkw"
      },
      "source": [
        "# Each worker has only one class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NArBViQXWpiv"
      },
      "source": [
        "# 10 Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TUCcazf52dZ"
      },
      "source": [
        "epochs = 2\n",
        "models = []\n",
        "i=0\n",
        "while(i<100):\n",
        "    models.append(nn.Sequential(*layers[i:i+10]))\n",
        "    i += 10"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqyBGX4JJEB5",
        "outputId": "440f0cb0-ac16-4832-80ba-45f49c86fd94"
      },
      "source": [
        "models"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sequential(\n",
              "   (0): Linear(in_features=3072, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=128, out_features=100, bias=True)\n",
              "   (9): LogSoftmax()\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-svbCqmP71zE"
      },
      "source": [
        "# Create optimisers for each segment and link to their segment\n",
        "optimizers = [\n",
        "    optim.SGD(model.parameters(), lr=0.03,)\n",
        "    for model in models\n",
        "]\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywuoVZ3b759u"
      },
      "source": [
        "\n",
        "# create some workers\n",
        "alices = []\n",
        "for i in range(10):\n",
        "    alices.append(sy.VirtualWorker(hook, id=\"alice\"+str(i+1)))\n",
        "\n",
        "workers = tuple(alices)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNmujk7-KGoM",
        "outputId": "de19d002-1f23-484e-fa2e-ab7e95c11b02"
      },
      "source": [
        "workers"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<VirtualWorker id:alice1 #objects:0>,\n",
              " <VirtualWorker id:alice2 #objects:0>,\n",
              " <VirtualWorker id:alice3 #objects:0>,\n",
              " <VirtualWorker id:alice4 #objects:0>,\n",
              " <VirtualWorker id:alice5 #objects:0>,\n",
              " <VirtualWorker id:alice6 #objects:0>,\n",
              " <VirtualWorker id:alice7 #objects:0>,\n",
              " <VirtualWorker id:alice8 #objects:0>,\n",
              " <VirtualWorker id:alice9 #objects:0>,\n",
              " <VirtualWorker id:alice10 #objects:0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vd1E0Bc79Ux"
      },
      "source": [
        "# Send Model Segments to starting locations\n",
        "model_locations = alices\n",
        "\n",
        "for model, location in zip(models, model_locations):\n",
        "    model.send(location)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-NUmxV3Kg8W"
      },
      "source": [
        "splitNN =  SplitNN(models, optimizers)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5K_xDUBKkQW"
      },
      "source": [
        "def train(x, target, splitNN):\n",
        "    \n",
        "    #1) Zero our grads\n",
        "    splitNN.zero_grads()\n",
        "    \n",
        "    #2) Make a prediction\n",
        "    pred = splitNN.forward(x)\n",
        "    # print(pred)\n",
        "    #3) Figure out how much we missed by\n",
        "    criterion = nn.NLLLoss()\n",
        "    loss = criterion(pred, target)\n",
        "  \n",
        "    #4) Backprop the loss on the end layer\n",
        "    loss.backward()\n",
        "    \n",
        "    #5) Feed Gradients backward through the network\n",
        "    splitNN.backward()\n",
        "    \n",
        "    #6) Change the weights\n",
        "    splitNN.step()\n",
        "    \n",
        "    return loss,pred"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQSSgzMocPaI"
      },
      "source": [
        "def test(x, target, splitNN):\n",
        "    \n",
        "    #1) Zero our grads\n",
        "    # splitNN.zero_grads()\n",
        "    \n",
        "    #2) Make a prediction\n",
        "    pred = splitNN.forward(x)\n",
        "    \n",
        "    #3) Figure out how much we missed by\n",
        "    # criterion = nn.NLLLoss()\n",
        "    # loss = criterion(pred, target)\n",
        "    \n",
        "    \n",
        "    return pred"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "brUhAFSCOpaC",
        "outputId": "3212b69f-c1ce-4c08-fd83-79d65b1a8fc1"
      },
      "source": [
        "models[-1].location.id"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'alice10'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgwdJsTcKo1b",
        "outputId": "6a265911-616f-4da8-8ad2-d1dab330adbf"
      },
      "source": [
        "%%time\n",
        "for i in range(epochs):\n",
        "    running_loss = 0\n",
        "    acc = 0\n",
        "    for images, labels in trainloader:\n",
        "        loss = 0\n",
        "        acc_c = 0\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        loss,pred = train(images, labels, splitNN)\n",
        "        # print(pred.shape)\n",
        "        # print(labels.shape)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        acc_c = train_acc/len(labels)\n",
        "        # print(train_acc.get()) \n",
        "        running_loss += loss.get()\n",
        "        acc +=acc_c\n",
        "\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {} Training Accuracy: {}\".format(i, running_loss/len(trainloader),acc/len(trainloader)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Training loss: 4.026189804077148 Training Accuracy: 0.07217071611253197\n",
            "Epoch 1 - Training loss: 3.626110553741455 Training Accuracy: 0.1257992327365729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1PGqscybIYi",
        "outputId": "4be5e028-e9e1-4a0e-c0c0-5595d8756719"
      },
      "source": [
        "testset = datasets.CIFAR100('cifar100', download=True, train=False, transform=transform)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yve2xl5bTMr",
        "outputId": "0cab2611-bac6-44a8-f8e3-e45d34064191"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(testset,shuffle=False)\n",
        "\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f12c573e410>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM23_IazsVUP"
      },
      "source": [
        "### Testing the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VG7b4X1Wzw_",
        "outputId": "3e0d7d4e-3725-45eb-a228-0ebba31aad4a"
      },
      "source": [
        "# loss=0\n",
        "acc_c = 0\n",
        "acc = 0\n",
        "with torch.no_grad():\n",
        " for images, labels in testloader:\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        pred = test(images, labels, splitNN)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        # acc_c = train_acc/len(labels)\n",
        "        # print(images.shape,labels.shape)\n",
        "        acc +=train_acc\n",
        "        # running_loss += loss.get()\n",
        "print(\"Testing accuracy: {}\".format(acc/len(testloader)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx-XYskPuF-6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpPJA5DMuMTF"
      },
      "source": [
        "# 20 Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TctG8CjSbaGY",
        "outputId": "7b9b74ff-1505-4fab-e1f1-79f71ef7c39a"
      },
      "source": [
        "100/20"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_MRJToguMTG"
      },
      "source": [
        "epochs = 3\n",
        "models = []\n",
        "i=0\n",
        "while(i<100):\n",
        "    models.append(nn.Sequential(*layers[i:i+5]))\n",
        "    i += 5"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVHoPCScuMTG",
        "outputId": "e0af15ea-57dd-4af4-cdef-5caedfdc95c2"
      },
      "source": [
        "models"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sequential(\n",
              "   (0): Linear(in_features=3072, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=128, out_features=100, bias=True)\n",
              "   (4): LogSoftmax()\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SetvZVcouMTG"
      },
      "source": [
        "# Create optimisers for each segment and link to their segment\n",
        "optimizers = [\n",
        "    optim.SGD(model.parameters(), lr=0.03,)\n",
        "    for model in models\n",
        "]\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7hGel5CuMTG"
      },
      "source": [
        "\n",
        "# create some workers\n",
        "alices = []\n",
        "for i in range(20):\n",
        "    alices.append(sy.VirtualWorker(hook, id=\"alice\"+str(i+1)))\n",
        "\n",
        "workers = tuple(alices)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk91m4XLuMTH",
        "outputId": "091037e8-bdf9-4a9c-fd20-141fd150436e"
      },
      "source": [
        "workers"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<VirtualWorker id:alice1 #objects:0>,\n",
              " <VirtualWorker id:alice2 #objects:0>,\n",
              " <VirtualWorker id:alice3 #objects:0>,\n",
              " <VirtualWorker id:alice4 #objects:0>,\n",
              " <VirtualWorker id:alice5 #objects:0>,\n",
              " <VirtualWorker id:alice6 #objects:0>,\n",
              " <VirtualWorker id:alice7 #objects:0>,\n",
              " <VirtualWorker id:alice8 #objects:0>,\n",
              " <VirtualWorker id:alice9 #objects:0>,\n",
              " <VirtualWorker id:alice10 #objects:0>,\n",
              " <VirtualWorker id:alice11 #objects:0>,\n",
              " <VirtualWorker id:alice12 #objects:0>,\n",
              " <VirtualWorker id:alice13 #objects:0>,\n",
              " <VirtualWorker id:alice14 #objects:0>,\n",
              " <VirtualWorker id:alice15 #objects:0>,\n",
              " <VirtualWorker id:alice16 #objects:0>,\n",
              " <VirtualWorker id:alice17 #objects:0>,\n",
              " <VirtualWorker id:alice18 #objects:0>,\n",
              " <VirtualWorker id:alice19 #objects:0>,\n",
              " <VirtualWorker id:alice20 #objects:0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1WnGDhruMTH"
      },
      "source": [
        "# Send Model Segments to starting locations\n",
        "model_locations = alices\n",
        "\n",
        "for model, location in zip(models, model_locations):\n",
        "    model.send(location)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2nVy2hCuMTH"
      },
      "source": [
        "splitNN =  SplitNN(models, optimizers)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ1-gEzduMTH"
      },
      "source": [
        "def train(x, target, splitNN):\n",
        "    \n",
        "    #1) Zero our grads\n",
        "    splitNN.zero_grads()\n",
        "    \n",
        "    #2) Make a prediction\n",
        "    pred = splitNN.forward(x)\n",
        "    # print(pred)\n",
        "    #3) Figure out how much we missed by\n",
        "    criterion = nn.NLLLoss()\n",
        "    loss = criterion(pred, target)\n",
        "  \n",
        "    #4) Backprop the loss on the end layer\n",
        "    loss.backward()\n",
        "    \n",
        "    #5) Feed Gradients backward through the network\n",
        "    splitNN.backward()\n",
        "    \n",
        "    #6) Change the weights\n",
        "    splitNN.step()\n",
        "    \n",
        "    return loss,pred"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAAus7M_uMTH"
      },
      "source": [
        "def test(x, target, splitNN):\n",
        "    \n",
        "    pred = splitNN.forward(x)\n",
        "    \n",
        "    return pred"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-f7WwppuuMTH",
        "outputId": "4d548209-92df-43be-8d8d-b629c22f054f"
      },
      "source": [
        "models[-1].location.id"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'alice20'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BRiG1pQuMTH",
        "outputId": "c5e6ab72-c88a-4f12-9c62-b288afbbb2f5"
      },
      "source": [
        "%%time\n",
        "for i in range(epochs):\n",
        "    running_loss = 0\n",
        "    acc = 0\n",
        "    for images, labels in trainloader:\n",
        "        loss = 0\n",
        "        acc_c = 0\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        loss,pred = train(images, labels, splitNN)\n",
        "        # print(pred.shape)\n",
        "        # print(labels.shape)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        acc_c = train_acc/len(labels)\n",
        "        # print(train_acc.get()) \n",
        "        running_loss += loss.get()\n",
        "        acc +=acc_c\n",
        "\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {} Training Accuracy: {}\".format(i, running_loss/len(trainloader),acc/len(trainloader)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Training loss: 4.026189804077148 Training Accuracy: 0.07217071611253197\n",
            "Epoch 1 - Training loss: 3.626110553741455 Training Accuracy: 0.1257992327365729\n",
            "Epoch 2 - Training loss: 3.262223482131958 Training Accuracy: 0.20076726342711\n",
            "CPU times: user 13min 51s, sys: 3min 51s, total: 17min 43s\n",
            "Wall time: 17min 45s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssOYbvctuMTH",
        "outputId": "1cde2be7-f0a7-4661-fcf0-70d8c8d4c822"
      },
      "source": [
        "testset = datasets.CIFAR100('cifar100', download=True, train=False, transform=transform)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bjiaL28uMTH",
        "outputId": "7f396ce0-c09e-4a51-9752-f281a3483c6f"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(testset,shuffle=False)\n",
        "\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9cdd247bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEDltE0OuMTH"
      },
      "source": [
        "### Testing the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl0-gBcNuMTI",
        "outputId": "1fc42481-d8dd-4044-8ebc-3d909cf8d1de"
      },
      "source": [
        "# loss=0\n",
        "%%time\n",
        "acc_c = 0\n",
        "acc = 0\n",
        "with torch.no_grad():\n",
        " for images, labels in testloader:\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        pred = test(images, labels, splitNN)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        # acc_c = train_acc/len(labels)\n",
        "        # print(images.shape,labels.shape)\n",
        "        acc +=train_acc\n",
        "        # running_loss += loss.get()\n",
        "print(\"Testing accuracy: {}\".format(acc/len(testloader)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy: 0.01\n",
            "CPU times: user 14min 18s, sys: 43.9 s, total: 15min 1s\n",
            "Wall time: 15min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ2I9x49uMTI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAhJ8Hmcv9NF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LO8a06rv9jv"
      },
      "source": [
        "# 50 Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDgDTvppv9jv"
      },
      "source": [
        "epochs = 3\n",
        "models = []\n",
        "i=0\n",
        "while(i<100):\n",
        "    models.append(nn.Sequential(*layers[i:i+2]))\n",
        "    i += 2"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzJ3Wv2kv9jw",
        "outputId": "1ee40225-19d9-4e7e-8ad8-5554a734feae"
      },
      "source": [
        "models"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sequential(\n",
              "   (0): Linear(in_features=3072, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=100, bias=True)\n",
              "   (1): LogSoftmax()\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnLKh1cYv9jw"
      },
      "source": [
        "# Create optimisers for each segment and link to their segment\n",
        "optimizers = [\n",
        "    optim.SGD(model.parameters(), lr=0.03,)\n",
        "    for model in models\n",
        "]\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNJ0tzY1v9jw"
      },
      "source": [
        "workers = []\n",
        "# create some workers\n",
        "alices = []\n",
        "for i in range(50):\n",
        "    alices.append(sy.VirtualWorker(hook, id=\"alice\"+str(i+1)))\n",
        "\n",
        "workers = tuple(alices)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9-i5pPTv9jw",
        "outputId": "4b2be19b-464c-412d-f5c4-1b5635a2b0b7"
      },
      "source": [
        "workers"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<VirtualWorker id:alice1 #objects:0>,\n",
              " <VirtualWorker id:alice2 #objects:0>,\n",
              " <VirtualWorker id:alice3 #objects:0>,\n",
              " <VirtualWorker id:alice4 #objects:0>,\n",
              " <VirtualWorker id:alice5 #objects:0>,\n",
              " <VirtualWorker id:alice6 #objects:0>,\n",
              " <VirtualWorker id:alice7 #objects:0>,\n",
              " <VirtualWorker id:alice8 #objects:0>,\n",
              " <VirtualWorker id:alice9 #objects:0>,\n",
              " <VirtualWorker id:alice10 #objects:0>,\n",
              " <VirtualWorker id:alice11 #objects:0>,\n",
              " <VirtualWorker id:alice12 #objects:0>,\n",
              " <VirtualWorker id:alice13 #objects:0>,\n",
              " <VirtualWorker id:alice14 #objects:0>,\n",
              " <VirtualWorker id:alice15 #objects:0>,\n",
              " <VirtualWorker id:alice16 #objects:0>,\n",
              " <VirtualWorker id:alice17 #objects:0>,\n",
              " <VirtualWorker id:alice18 #objects:0>,\n",
              " <VirtualWorker id:alice19 #objects:0>,\n",
              " <VirtualWorker id:alice20 #objects:0>,\n",
              " <VirtualWorker id:alice21 #objects:0>,\n",
              " <VirtualWorker id:alice22 #objects:0>,\n",
              " <VirtualWorker id:alice23 #objects:0>,\n",
              " <VirtualWorker id:alice24 #objects:0>,\n",
              " <VirtualWorker id:alice25 #objects:0>,\n",
              " <VirtualWorker id:alice26 #objects:0>,\n",
              " <VirtualWorker id:alice27 #objects:0>,\n",
              " <VirtualWorker id:alice28 #objects:0>,\n",
              " <VirtualWorker id:alice29 #objects:0>,\n",
              " <VirtualWorker id:alice30 #objects:0>,\n",
              " <VirtualWorker id:alice31 #objects:0>,\n",
              " <VirtualWorker id:alice32 #objects:0>,\n",
              " <VirtualWorker id:alice33 #objects:0>,\n",
              " <VirtualWorker id:alice34 #objects:0>,\n",
              " <VirtualWorker id:alice35 #objects:0>,\n",
              " <VirtualWorker id:alice36 #objects:0>,\n",
              " <VirtualWorker id:alice37 #objects:0>,\n",
              " <VirtualWorker id:alice38 #objects:0>,\n",
              " <VirtualWorker id:alice39 #objects:0>,\n",
              " <VirtualWorker id:alice40 #objects:0>,\n",
              " <VirtualWorker id:alice41 #objects:0>,\n",
              " <VirtualWorker id:alice42 #objects:0>,\n",
              " <VirtualWorker id:alice43 #objects:0>,\n",
              " <VirtualWorker id:alice44 #objects:0>,\n",
              " <VirtualWorker id:alice45 #objects:0>,\n",
              " <VirtualWorker id:alice46 #objects:0>,\n",
              " <VirtualWorker id:alice47 #objects:0>,\n",
              " <VirtualWorker id:alice48 #objects:0>,\n",
              " <VirtualWorker id:alice49 #objects:0>,\n",
              " <VirtualWorker id:alice50 #objects:0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1PbPCmnv9jw"
      },
      "source": [
        "# Send Model Segments to starting locations\n",
        "# model_locations = []\n",
        "model_locations = alices\n",
        "\n",
        "for model, location in zip(models, model_locations):\n",
        "    model.send(location)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayZRgKuAv9jw"
      },
      "source": [
        "splitNN =  SplitNN(models, optimizers)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FSWQOEDv9jw"
      },
      "source": [
        "def train(x, target, splitNN):\n",
        "    \n",
        "    #1) Zero our grads\n",
        "    splitNN.zero_grads()\n",
        "    \n",
        "    #2) Make a prediction\n",
        "    pred = splitNN.forward(x)\n",
        "    # print(pred)\n",
        "    #3) Figure out how much we missed by\n",
        "    criterion = nn.NLLLoss()\n",
        "    loss = criterion(pred, target)\n",
        "  \n",
        "    #4) Backprop the loss on the end layer\n",
        "    loss.backward()\n",
        "    \n",
        "    #5) Feed Gradients backward through the network\n",
        "    splitNN.backward()\n",
        "    \n",
        "    #6) Change the weights\n",
        "    splitNN.step()\n",
        "    \n",
        "    return loss,pred"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyB3fQ8cv9jx"
      },
      "source": [
        "def test(x, target, splitNN):\n",
        "    \n",
        "    pred = splitNN.forward(x)\n",
        "    \n",
        "    return pred"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VspMmqxZv9jx",
        "outputId": "be3ebf23-388d-4c1d-f8b5-d0f9b322a65f"
      },
      "source": [
        "models[-1].location.id"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'alice50'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DZgvOSVv9jx",
        "outputId": "db7956a1-a097-4f38-8538-4fe55fee9435"
      },
      "source": [
        "%%time\n",
        "for i in range(epochs):\n",
        "    running_loss = 0\n",
        "    acc = 0\n",
        "    for images, labels in trainloader:\n",
        "        loss = 0\n",
        "        acc_c = 0\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        loss,pred = train(images, labels, splitNN)\n",
        "        # print(pred.shape)\n",
        "        # print(labels.shape)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        acc_c = train_acc/len(labels)\n",
        "        # print(train_acc.get()) \n",
        "        running_loss += loss.get()\n",
        "        acc +=acc_c\n",
        "\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {} Training Accuracy: {}\".format(i, running_loss/len(trainloader),acc/len(trainloader)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Training loss: 4.026189804077148 Training Accuracy: 0.07217071611253197\n",
            "Epoch 1 - Training loss: 3.626110553741455 Training Accuracy: 0.1257992327365729\n",
            "Epoch 2 - Training loss: 3.262223482131958 Training Accuracy: 0.20076726342711\n",
            "CPU times: user 21min 31s, sys: 6min 31s, total: 28min 2s\n",
            "Wall time: 28min 4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk8vJVhwv9jx",
        "outputId": "25b0d9b2-b6b8-4f8b-8452-748df67ba49e"
      },
      "source": [
        "testset = datasets.CIFAR100('cifar100', download=True, train=False, transform=transform)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhfI9_k0v9jx",
        "outputId": "0485daad-a671-459c-efa7-dcdce6bfd819"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(testset,shuffle=False)\n",
        "\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f841b5b6bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-E1S6zov9jx"
      },
      "source": [
        "### Testing the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KseHZU64v9jx",
        "outputId": "8c6025be-84af-45a5-8dd1-97d446cb49cd"
      },
      "source": [
        "%%time\n",
        "# loss=0\n",
        "acc_c = 0\n",
        "acc = 0\n",
        "with torch.no_grad():\n",
        " for images, labels in testloader:\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        pred = test(images, labels, splitNN)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        # acc_c = train_acc/len(labels)\n",
        "        # print(images.shape,labels.shape)\n",
        "        acc +=train_acc\n",
        "        # running_loss += loss.get()\n",
        "print(\"Testing accuracy: {}\".format(acc/len(testloader)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy: 0.01\n",
            "CPU times: user 29min 37s, sys: 17min 40s, total: 47min 17s\n",
            "Wall time: 47min 14s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgFyhRTzv9jx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4oUosywwvoq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJe1PqP7ix7v"
      },
      "source": [
        "# 100 Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8a7gSZyix7w"
      },
      "source": [
        "\n",
        "epochs = 2\n",
        "\n",
        "# Define our model segments\n",
        "models = []\n",
        "input_size = 3072\n",
        "hidden_sizes = [128, 640]\n",
        "output_size = 100\n",
        "inp = input_size\n",
        "out = hidden_sizes[0]\n",
        "k=0\n",
        "for i in range(100):\n",
        "\n",
        "                models.append(nn.Sequential(\n",
        "                            nn.Linear(inp, out),\n",
        "                            nn.ReLU(),\n",
        "                ))\n",
        "                inp = hidden_sizes[k]\n",
        "                if k==0:\n",
        "                    k=1\n",
        "                else:\n",
        "                    k=0\n",
        "                out = hidden_sizes[k]\n",
        "models.append(nn.Sequential(\n",
        "                nn.Linear(inp, output_size),\n",
        "                nn.LogSoftmax(dim=1)\n",
        "    ))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kfnzN4six7w",
        "outputId": "a772810e-1faa-4d21-8a0b-27f0e40a06fd"
      },
      "source": [
        "models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sequential(\n",
              "   (0): Linear(in_features=3072, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=100, bias=True)\n",
              "   (1): LogSoftmax()\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxblRri8ix7w"
      },
      "source": [
        "# Create optimisers for each segment and link to their segment\n",
        "optimizers = [\n",
        "    optim.SGD(model.parameters(), lr=0.03,)\n",
        "    for model in models\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6bP5cEhix7w"
      },
      "source": [
        "\n",
        "# create some workers\n",
        "alices = []\n",
        "for i in range(101):\n",
        "    alices.append(sy.VirtualWorker(hook, id=\"alice\"+str(i+1)))\n",
        "\n",
        "workers = tuple(alices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywVuo2h1ix7w",
        "outputId": "7c1298cb-1605-4038-ae42-99e6871f3a39"
      },
      "source": [
        "workers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<VirtualWorker id:alice1 #objects:6>,\n",
              " <VirtualWorker id:alice2 #objects:6>,\n",
              " <VirtualWorker id:alice3 #objects:6>,\n",
              " <VirtualWorker id:alice4 #objects:6>,\n",
              " <VirtualWorker id:alice5 #objects:6>,\n",
              " <VirtualWorker id:alice6 #objects:6>,\n",
              " <VirtualWorker id:alice7 #objects:6>,\n",
              " <VirtualWorker id:alice8 #objects:6>,\n",
              " <VirtualWorker id:alice9 #objects:6>,\n",
              " <VirtualWorker id:alice10 #objects:6>,\n",
              " <VirtualWorker id:alice11 #objects:6>,\n",
              " <VirtualWorker id:alice12 #objects:6>,\n",
              " <VirtualWorker id:alice13 #objects:6>,\n",
              " <VirtualWorker id:alice14 #objects:6>,\n",
              " <VirtualWorker id:alice15 #objects:6>,\n",
              " <VirtualWorker id:alice16 #objects:6>,\n",
              " <VirtualWorker id:alice17 #objects:6>,\n",
              " <VirtualWorker id:alice18 #objects:6>,\n",
              " <VirtualWorker id:alice19 #objects:6>,\n",
              " <VirtualWorker id:alice20 #objects:6>,\n",
              " <VirtualWorker id:alice21 #objects:6>,\n",
              " <VirtualWorker id:alice22 #objects:6>,\n",
              " <VirtualWorker id:alice23 #objects:6>,\n",
              " <VirtualWorker id:alice24 #objects:6>,\n",
              " <VirtualWorker id:alice25 #objects:6>,\n",
              " <VirtualWorker id:alice26 #objects:6>,\n",
              " <VirtualWorker id:alice27 #objects:6>,\n",
              " <VirtualWorker id:alice28 #objects:6>,\n",
              " <VirtualWorker id:alice29 #objects:6>,\n",
              " <VirtualWorker id:alice30 #objects:6>,\n",
              " <VirtualWorker id:alice31 #objects:6>,\n",
              " <VirtualWorker id:alice32 #objects:6>,\n",
              " <VirtualWorker id:alice33 #objects:6>,\n",
              " <VirtualWorker id:alice34 #objects:6>,\n",
              " <VirtualWorker id:alice35 #objects:6>,\n",
              " <VirtualWorker id:alice36 #objects:6>,\n",
              " <VirtualWorker id:alice37 #objects:6>,\n",
              " <VirtualWorker id:alice38 #objects:6>,\n",
              " <VirtualWorker id:alice39 #objects:6>,\n",
              " <VirtualWorker id:alice40 #objects:6>,\n",
              " <VirtualWorker id:alice41 #objects:6>,\n",
              " <VirtualWorker id:alice42 #objects:6>,\n",
              " <VirtualWorker id:alice43 #objects:6>,\n",
              " <VirtualWorker id:alice44 #objects:6>,\n",
              " <VirtualWorker id:alice45 #objects:6>,\n",
              " <VirtualWorker id:alice46 #objects:6>,\n",
              " <VirtualWorker id:alice47 #objects:6>,\n",
              " <VirtualWorker id:alice48 #objects:6>,\n",
              " <VirtualWorker id:alice49 #objects:6>,\n",
              " <VirtualWorker id:alice50 #objects:6>,\n",
              " <VirtualWorker id:alice51 #objects:6>,\n",
              " <VirtualWorker id:alice52 #objects:6>,\n",
              " <VirtualWorker id:alice53 #objects:6>,\n",
              " <VirtualWorker id:alice54 #objects:6>,\n",
              " <VirtualWorker id:alice55 #objects:6>,\n",
              " <VirtualWorker id:alice56 #objects:6>,\n",
              " <VirtualWorker id:alice57 #objects:6>,\n",
              " <VirtualWorker id:alice58 #objects:6>,\n",
              " <VirtualWorker id:alice59 #objects:6>,\n",
              " <VirtualWorker id:alice60 #objects:6>,\n",
              " <VirtualWorker id:alice61 #objects:6>,\n",
              " <VirtualWorker id:alice62 #objects:6>,\n",
              " <VirtualWorker id:alice63 #objects:6>,\n",
              " <VirtualWorker id:alice64 #objects:6>,\n",
              " <VirtualWorker id:alice65 #objects:6>,\n",
              " <VirtualWorker id:alice66 #objects:6>,\n",
              " <VirtualWorker id:alice67 #objects:6>,\n",
              " <VirtualWorker id:alice68 #objects:6>,\n",
              " <VirtualWorker id:alice69 #objects:6>,\n",
              " <VirtualWorker id:alice70 #objects:6>,\n",
              " <VirtualWorker id:alice71 #objects:6>,\n",
              " <VirtualWorker id:alice72 #objects:6>,\n",
              " <VirtualWorker id:alice73 #objects:6>,\n",
              " <VirtualWorker id:alice74 #objects:6>,\n",
              " <VirtualWorker id:alice75 #objects:6>,\n",
              " <VirtualWorker id:alice76 #objects:6>,\n",
              " <VirtualWorker id:alice77 #objects:6>,\n",
              " <VirtualWorker id:alice78 #objects:6>,\n",
              " <VirtualWorker id:alice79 #objects:6>,\n",
              " <VirtualWorker id:alice80 #objects:6>,\n",
              " <VirtualWorker id:alice81 #objects:6>,\n",
              " <VirtualWorker id:alice82 #objects:6>,\n",
              " <VirtualWorker id:alice83 #objects:6>,\n",
              " <VirtualWorker id:alice84 #objects:6>,\n",
              " <VirtualWorker id:alice85 #objects:6>,\n",
              " <VirtualWorker id:alice86 #objects:6>,\n",
              " <VirtualWorker id:alice87 #objects:6>,\n",
              " <VirtualWorker id:alice88 #objects:6>,\n",
              " <VirtualWorker id:alice89 #objects:6>,\n",
              " <VirtualWorker id:alice90 #objects:6>,\n",
              " <VirtualWorker id:alice91 #objects:6>,\n",
              " <VirtualWorker id:alice92 #objects:6>,\n",
              " <VirtualWorker id:alice93 #objects:6>,\n",
              " <VirtualWorker id:alice94 #objects:6>,\n",
              " <VirtualWorker id:alice95 #objects:6>,\n",
              " <VirtualWorker id:alice96 #objects:6>,\n",
              " <VirtualWorker id:alice97 #objects:6>,\n",
              " <VirtualWorker id:alice98 #objects:6>,\n",
              " <VirtualWorker id:alice99 #objects:6>,\n",
              " <VirtualWorker id:alice100 #objects:6>,\n",
              " <VirtualWorker id:alice101 #objects:6>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyXVQSkWix7w"
      },
      "source": [
        "# Send Model Segments to starting locations\n",
        "model_locations = alices\n",
        "\n",
        "for model, location in zip(models, model_locations):\n",
        "    model.send(location)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owbR-KHBix7x"
      },
      "source": [
        "splitNN =  SplitNN(models, optimizers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6kvWHsKix7x"
      },
      "source": [
        "def train(x, target, splitNN):\n",
        "    \n",
        "    #1) Zero our grads\n",
        "    splitNN.zero_grads()\n",
        "    \n",
        "    #2) Make a prediction\n",
        "    pred = splitNN.forward(x)\n",
        "    # print(pred)\n",
        "    #3) Figure out how much we missed by\n",
        "    criterion = nn.NLLLoss()\n",
        "    loss = criterion(pred, target)\n",
        "  \n",
        "    #4) Backprop the loss on the end layer\n",
        "    loss.backward()\n",
        "    \n",
        "    #5) Feed Gradients backward through the network\n",
        "    splitNN.backward()\n",
        "    \n",
        "    #6) Change the weights\n",
        "    splitNN.step()\n",
        "    \n",
        "    return loss,pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuO8Wghoix7x"
      },
      "source": [
        "def test(x, target, splitNN):\n",
        "    \n",
        "    pred = splitNN.forward(x)\n",
        "    \n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6eckbXE4ix7x",
        "outputId": "a097414d-b091-4fb6-e1e7-f34f1a6b8c99"
      },
      "source": [
        "models[-1].location.id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'alice101'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDms7ymSix7x",
        "outputId": "d7f35f09-b0e3-4bf3-81f2-6e2f3340a92e"
      },
      "source": [
        "for i in range(epochs):\n",
        "    running_loss = 0\n",
        "    acc = 0\n",
        "    for images, labels in trainloader:\n",
        "        loss = 0\n",
        "        acc_c = 0\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        loss,pred = train(images, labels, splitNN)\n",
        "        # print(pred.shape)\n",
        "        # print(labels.shape)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        acc_c = train_acc/len(labels)\n",
        "        # print(train_acc.get()) \n",
        "        running_loss += loss.get()\n",
        "        acc +=acc_c\n",
        "\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {} Training Accuracy: {}\".format(i, running_loss/len(trainloader),acc/len(trainloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Training loss: 3.7677416801452637 Training Accuracy: 0.12228260869565218\n",
            "Epoch 1 - Training loss: 3.2349305152893066 Training Accuracy: 0.2108375959079284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InPQEeV_ix7x",
        "outputId": "aec6c3e0-56c0-4152-f071-0db0b5aa21cf"
      },
      "source": [
        "testset = datasets.CIFAR100('cifar100', download=True, train=False, transform=transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YylSqxeYix7x",
        "outputId": "985e84f5-26c8-423d-99e0-065088e522c2"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(testset,shuffle=False)\n",
        "\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f865b46e9f0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtiMOe7zix7x"
      },
      "source": [
        "### Testing the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxONWrg6ix7x",
        "outputId": "5339fcf7-88ba-4ae5-8e23-b1d240d9396a"
      },
      "source": [
        "# loss=0\n",
        "acc_c = 0\n",
        "acc = 0\n",
        "with torch.no_grad():\n",
        " for images, labels in testloader:\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        pred = test(images, labels, splitNN)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        # acc_c = train_acc/len(labels)\n",
        "        # print(images.shape,labels.shape)\n",
        "        acc +=train_acc\n",
        "        # running_loss += loss.get()\n",
        "print(\"Testing accuracy: {}\".format(acc/len(testloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZCsUGFOix7y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq2XM-IYix7y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_IC2bRh_9xn"
      },
      "source": [
        "# Every worker has all the classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLJd1sAsAHiQ"
      },
      "source": [
        "x_new = []\n",
        "y_new = []\n",
        "for i in range(500):\n",
        "    for j in range(x_train.shape[0]//500):\n",
        "        x_new.append(x_train[j*500+i])\n",
        "        y_new.append(y_train[j*500+i])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4MLXsXjoch7"
      },
      "source": [
        "trainset.data = np.array(x_new)\n",
        "trainset.targets = np.array(y_new)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzCm1ESzohOw",
        "outputId": "a82eb9ca-87bf-4423-a521-eff419599641"
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False)\n",
        "\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9b37b73e90>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ag8FUZqokWP",
        "outputId": "b8852b98-903f-4210-a77e-a10da797b422"
      },
      "source": [
        "trainloader.dataset.targets"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2, ..., 97, 98, 99])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nc-eXVKAHwO"
      },
      "source": [
        "# 10 Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vv-NyW6AHwO"
      },
      "source": [
        "epochs = 2\n",
        "models = []\n",
        "i=0\n",
        "while(i<100):\n",
        "    models.append(nn.Sequential(*layers[i:i+10]))\n",
        "    i += 10"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj5eGB5TAHwO",
        "outputId": "4ff5c3a1-d965-4127-d2f5-9ce897bc7dea"
      },
      "source": [
        "models"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sequential(\n",
              "   (0): Linear(in_features=3072, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=128, out_features=100, bias=True)\n",
              "   (9): LogSoftmax()\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ-1NjhyAHwP"
      },
      "source": [
        "# Create optimisers for each segment and link to their segment\n",
        "optimizers = [\n",
        "    optim.SGD(model.parameters(), lr=0.03,)\n",
        "    for model in models\n",
        "]\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5ZzyGnUAHwP"
      },
      "source": [
        "\n",
        "# create some workers\n",
        "alices = []\n",
        "for i in range(11):\n",
        "    alices.append(sy.VirtualWorker(hook, id=\"alice\"+str(i+1)))\n",
        "\n",
        "workers = tuple(alices)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JQEIQobAHwP",
        "outputId": "528d8b01-4746-41b3-9b96-381815873422"
      },
      "source": [
        "workers"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<VirtualWorker id:alice1 #objects:0>,\n",
              " <VirtualWorker id:alice2 #objects:0>,\n",
              " <VirtualWorker id:alice3 #objects:0>,\n",
              " <VirtualWorker id:alice4 #objects:0>,\n",
              " <VirtualWorker id:alice5 #objects:0>,\n",
              " <VirtualWorker id:alice6 #objects:0>,\n",
              " <VirtualWorker id:alice7 #objects:0>,\n",
              " <VirtualWorker id:alice8 #objects:0>,\n",
              " <VirtualWorker id:alice9 #objects:0>,\n",
              " <VirtualWorker id:alice10 #objects:0>,\n",
              " <VirtualWorker id:alice11 #objects:0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OkSbVACAHwP"
      },
      "source": [
        "# Send Model Segments to starting locations\n",
        "model_locations = alices\n",
        "\n",
        "for model, location in zip(models, model_locations):\n",
        "    model.send(location)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qziPhXRIAHwP"
      },
      "source": [
        "splitNN =  SplitNN(models, optimizers)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIIA1g_WAHwP"
      },
      "source": [
        "def train(x, target, splitNN):\n",
        "    \n",
        "    #1) Zero our grads\n",
        "    splitNN.zero_grads()\n",
        "    \n",
        "    #2) Make a prediction\n",
        "    pred = splitNN.forward(x)\n",
        "    # print(pred)\n",
        "    #3) Figure out how much we missed by\n",
        "    criterion = nn.NLLLoss()\n",
        "    loss = criterion(pred, target)\n",
        "  \n",
        "    #4) Backprop the loss on the end layer\n",
        "    loss.backward()\n",
        "    \n",
        "    #5) Feed Gradients backward through the network\n",
        "    splitNN.backward()\n",
        "    \n",
        "    #6) Change the weights\n",
        "    splitNN.step()\n",
        "    \n",
        "    return loss,pred"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVc-bINcAHwP"
      },
      "source": [
        "def test(x, target, splitNN):\n",
        "    \n",
        "    #1) Zero our grads\n",
        "    # splitNN.zero_grads()\n",
        "    \n",
        "    #2) Make a prediction\n",
        "    pred = splitNN.forward(x)\n",
        "    \n",
        "    #3) Figure out how much we missed by\n",
        "    # criterion = nn.NLLLoss()\n",
        "    # loss = criterion(pred, target)\n",
        "    \n",
        "    \n",
        "    return pred"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uFwuFqVhAHwP",
        "outputId": "0f3ccaf6-8c2a-4945-cfd7-b407412af088"
      },
      "source": [
        "models[-1].location.id"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'alice10'"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "341bdH3cAHwQ",
        "outputId": "6e82cd2c-4263-4e41-9f2c-270c2bb8f788"
      },
      "source": [
        "%%time\n",
        "for i in range(epochs):\n",
        "    running_loss = 0\n",
        "    acc = 0\n",
        "    for images, labels in trainloader:\n",
        "        loss = 0\n",
        "        acc_c = 0\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        loss,pred = train(images, labels, splitNN)\n",
        "        # print(pred.shape)\n",
        "        # print(labels.shape)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        acc_c = train_acc/len(labels)\n",
        "        # print(train_acc.get()) \n",
        "        running_loss += loss.get()\n",
        "        acc +=acc_c\n",
        "\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {} Training Accuracy: {}\".format(i, running_loss/len(trainloader),acc/len(trainloader)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Training loss: 4.6063055992126465 Training Accuracy: 0.009990409207161126\n",
            "Epoch 1 - Training loss: 4.605844497680664 Training Accuracy: 0.009990409207161126\n",
            "CPU times: user 8min 9s, sys: 2min 15s, total: 10min 25s\n",
            "Wall time: 10min 25s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw4tzj51AHwQ",
        "outputId": "7f028738-f71b-4463-d112-633b92dc5a4b"
      },
      "source": [
        "testset = datasets.CIFAR100('cifar100', download=True, train=False, transform=transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaNdzHEtAHwQ",
        "outputId": "7958b640-0784-4479-edbe-859330514c90"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(testset,shuffle=False)\n",
        "\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f865b46e9f0>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmHBL6wjAHwQ"
      },
      "source": [
        "### Testing the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwFt1eV3AHwQ",
        "outputId": "6cc18fe1-b8f8-43ae-d813-2c241d693e72"
      },
      "source": [
        "# loss=0\n",
        "acc_c = 0\n",
        "acc = 0\n",
        "with torch.no_grad():\n",
        " for images, labels in testloader:\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        pred = test(images, labels, splitNN)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        # acc_c = train_acc/len(labels)\n",
        "        # print(images.shape,labels.shape)\n",
        "        acc +=train_acc\n",
        "        # running_loss += loss.get()\n",
        "print(\"Testing accuracy: {}\".format(acc/len(testloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy: 0.0107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oyp1lkJnAHwQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqZN-F7xAHwQ"
      },
      "source": [
        "# 20 Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I31DKlnRAHwR"
      },
      "source": [
        "\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "\n",
        "# Define our model segments\n",
        "models = []\n",
        "input_size = 3072\n",
        "hidden_sizes = [128, 640]\n",
        "output_size = 100\n",
        "inp = input_size\n",
        "out = hidden_sizes[0]\n",
        "k=0\n",
        "for i in range(20):\n",
        "\n",
        "                models.append(nn.Sequential(\n",
        "                            nn.Linear(inp, out),\n",
        "                            nn.ReLU(),\n",
        "                ))\n",
        "                inp = hidden_sizes[k]\n",
        "                if k==0:\n",
        "                    k=1\n",
        "                else:\n",
        "                    k=0\n",
        "                out = hidden_sizes[k]\n",
        "models.append(nn.Sequential(\n",
        "                nn.Linear(inp, output_size),\n",
        "                nn.LogSoftmax(dim=1)\n",
        "    ))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1deTEi6AHwR",
        "outputId": "a0c47856-8ddf-4f40-8ab1-213b3e51c47c"
      },
      "source": [
        "models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sequential(\n",
              "   (0): Linear(in_features=3072, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=100, bias=True)\n",
              "   (1): LogSoftmax()\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22KzYMuSAHwR"
      },
      "source": [
        "# Create optimisers for each segment and link to their segment\n",
        "optimizers = [\n",
        "    optim.SGD(model.parameters(), lr=0.03,)\n",
        "    for model in models\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUoLSco7AHwR"
      },
      "source": [
        "\n",
        "# create some workers\n",
        "alices = []\n",
        "for i in range(21):\n",
        "    alices.append(sy.VirtualWorker(hook, id=\"alice\"+str(i+1)))\n",
        "\n",
        "workers = tuple(alices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNvFJNHvAHwR",
        "outputId": "f18a3817-39ca-4abc-e3a4-47ac5a81ae3f"
      },
      "source": [
        "workers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<VirtualWorker id:alice1 #objects:24>,\n",
              " <VirtualWorker id:alice2 #objects:24>,\n",
              " <VirtualWorker id:alice3 #objects:24>,\n",
              " <VirtualWorker id:alice4 #objects:24>,\n",
              " <VirtualWorker id:alice5 #objects:24>,\n",
              " <VirtualWorker id:alice6 #objects:24>,\n",
              " <VirtualWorker id:alice7 #objects:24>,\n",
              " <VirtualWorker id:alice8 #objects:24>,\n",
              " <VirtualWorker id:alice9 #objects:24>,\n",
              " <VirtualWorker id:alice10 #objects:24>,\n",
              " <VirtualWorker id:alice11 #objects:26>,\n",
              " <VirtualWorker id:alice12 #objects:14>,\n",
              " <VirtualWorker id:alice13 #objects:14>,\n",
              " <VirtualWorker id:alice14 #objects:14>,\n",
              " <VirtualWorker id:alice15 #objects:14>,\n",
              " <VirtualWorker id:alice16 #objects:14>,\n",
              " <VirtualWorker id:alice17 #objects:14>,\n",
              " <VirtualWorker id:alice18 #objects:14>,\n",
              " <VirtualWorker id:alice19 #objects:14>,\n",
              " <VirtualWorker id:alice20 #objects:14>,\n",
              " <VirtualWorker id:alice21 #objects:14>)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqohP2DCAHwR"
      },
      "source": [
        "# Send Model Segments to starting locations\n",
        "model_locations = alices\n",
        "\n",
        "for model, location in zip(models, model_locations):\n",
        "    model.send(location)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWHGAb-RAHwR"
      },
      "source": [
        "splitNN =  SplitNN(models, optimizers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YYvkS4QAHwR"
      },
      "source": [
        "def train(x, target, splitNN):\n",
        "    \n",
        "    #1) Zero our grads\n",
        "    splitNN.zero_grads()\n",
        "    \n",
        "    #2) Make a prediction\n",
        "    pred = splitNN.forward(x)\n",
        "    # print(pred)\n",
        "    #3) Figure out how much we missed by\n",
        "    criterion = nn.NLLLoss()\n",
        "    loss = criterion(pred, target)\n",
        "  \n",
        "    #4) Backprop the loss on the end layer\n",
        "    loss.backward()\n",
        "    \n",
        "    #5) Feed Gradients backward through the network\n",
        "    splitNN.backward()\n",
        "    \n",
        "    #6) Change the weights\n",
        "    splitNN.step()\n",
        "    \n",
        "    return loss,pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-GV48PRAHwR"
      },
      "source": [
        "def test(x, target, splitNN):\n",
        "    \n",
        "    pred = splitNN.forward(x)\n",
        "    \n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NY8PQkwXAHwS",
        "outputId": "970e9212-aa2c-4c35-eb59-04e06af7d59e"
      },
      "source": [
        "models[-1].location.id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'alice21'"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLBC2GZyAHwS",
        "outputId": "3eb3787d-f078-4e44-81ac-9b629717eea5"
      },
      "source": [
        "for i in range(epochs):\n",
        "    running_loss = 0\n",
        "    acc = 0\n",
        "    for images, labels in trainloader:\n",
        "        loss = 0\n",
        "        acc_c = 0\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        loss,pred = train(images, labels, splitNN)\n",
        "        # print(pred.shape)\n",
        "        # print(labels.shape)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        acc_c = train_acc/len(labels)\n",
        "        # print(train_acc.get()) \n",
        "        running_loss += loss.get()\n",
        "        acc +=acc_c\n",
        "\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {} Training Accuracy: {}\".format(i, running_loss/len(trainloader),acc/len(trainloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Training loss: 4.605624675750732 Training Accuracy: 0.009990409207161126\n",
            "Epoch 1 - Training loss: 4.6054511070251465 Training Accuracy: 0.009990409207161126\n",
            "Epoch 2 - Training loss: 4.605381488800049 Training Accuracy: 0.009990409207161126\n",
            "Epoch 3 - Training loss: 4.605358600616455 Training Accuracy: 0.009910485933503837\n",
            "Epoch 4 - Training loss: 4.6053466796875 Training Accuracy: 0.010050351662404092\n",
            "Epoch 5 - Training loss: 4.605339050292969 Training Accuracy: 0.010050351662404092\n",
            "Epoch 6 - Training loss: 4.605331897735596 Training Accuracy: 0.009810581841432225\n",
            "Epoch 7 - Training loss: 4.605337619781494 Training Accuracy: 0.00941096547314578\n",
            "Epoch 8 - Training loss: 4.605342864990234 Training Accuracy: 0.00941096547314578\n",
            "Epoch 9 - Training loss: 4.605345249176025 Training Accuracy: 0.008951406649616368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTJ9qHv2AHwS",
        "outputId": "27758cb1-1595-48cf-b02d-229cf9706ca9"
      },
      "source": [
        "testset = datasets.CIFAR100('cifar100', download=True, train=False, transform=transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_MuYHENAHwS",
        "outputId": "c9ed9906-e766-457a-8ae9-8ab68faba8fc"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(testset,shuffle=False)\n",
        "\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f865b46e9f0>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8LB5jDPAHwS"
      },
      "source": [
        "### Testing the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2EYKBMcAHwS",
        "outputId": "c6d24b6c-4294-4b53-f8c0-598151a19bd8"
      },
      "source": [
        "# loss=0\n",
        "acc_c = 0\n",
        "acc = 0\n",
        "with torch.no_grad():\n",
        " for images, labels in testloader:\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        pred = test(images, labels, splitNN)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        # acc_c = train_acc/len(labels)\n",
        "        # print(images.shape,labels.shape)\n",
        "        acc +=train_acc\n",
        "        # running_loss += loss.get()\n",
        "print(\"Testing accuracy: {}\".format(acc/len(testloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Omh_Q6gAHwS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE70quPHAHwS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwEMPtFWAHwS"
      },
      "source": [
        "# 50 Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HhuHelzAHwT"
      },
      "source": [
        "\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "\n",
        "# Define our model segments\n",
        "models = []\n",
        "input_size = 3072\n",
        "hidden_sizes = [128, 640]\n",
        "output_size = 100\n",
        "inp = input_size\n",
        "out = hidden_sizes[0]\n",
        "k=0\n",
        "for i in range(50):\n",
        "\n",
        "                models.append(nn.Sequential(\n",
        "                            nn.Linear(inp, out),\n",
        "                            nn.ReLU(),\n",
        "                ))\n",
        "                inp = hidden_sizes[k]\n",
        "                if k==0:\n",
        "                    k=1\n",
        "                else:\n",
        "                    k=0\n",
        "                out = hidden_sizes[k]\n",
        "models.append(nn.Sequential(\n",
        "                nn.Linear(inp, output_size),\n",
        "                nn.LogSoftmax(dim=1)\n",
        "    ))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akV3oaxXAHwT",
        "outputId": "af4e7dec-68d4-463f-f107-5aada27355b9"
      },
      "source": [
        "models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sequential(\n",
              "   (0): Linear(in_features=3072, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=100, bias=True)\n",
              "   (1): LogSoftmax()\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w_2PLWbAHwT"
      },
      "source": [
        "# Create optimisers for each segment and link to their segment\n",
        "optimizers = [\n",
        "    optim.SGD(model.parameters(), lr=0.03,)\n",
        "    for model in models\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KForHGJ4AHwT"
      },
      "source": [
        "\n",
        "# create some workers\n",
        "alices = []\n",
        "for i in range(51):\n",
        "    alices.append(sy.VirtualWorker(hook, id=\"alice\"+str(i+1)))\n",
        "\n",
        "workers = tuple(alices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-uNzgGKAHwT",
        "outputId": "d511366c-a4e6-4ccf-f3e1-afa8cc0e0eaf"
      },
      "source": [
        "workers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<VirtualWorker id:alice1 #objects:32>,\n",
              " <VirtualWorker id:alice2 #objects:32>,\n",
              " <VirtualWorker id:alice3 #objects:32>,\n",
              " <VirtualWorker id:alice4 #objects:32>,\n",
              " <VirtualWorker id:alice5 #objects:32>,\n",
              " <VirtualWorker id:alice6 #objects:32>,\n",
              " <VirtualWorker id:alice7 #objects:32>,\n",
              " <VirtualWorker id:alice8 #objects:32>,\n",
              " <VirtualWorker id:alice9 #objects:32>,\n",
              " <VirtualWorker id:alice10 #objects:32>,\n",
              " <VirtualWorker id:alice11 #objects:32>,\n",
              " <VirtualWorker id:alice12 #objects:24>,\n",
              " <VirtualWorker id:alice13 #objects:24>,\n",
              " <VirtualWorker id:alice14 #objects:24>,\n",
              " <VirtualWorker id:alice15 #objects:24>,\n",
              " <VirtualWorker id:alice16 #objects:24>,\n",
              " <VirtualWorker id:alice17 #objects:24>,\n",
              " <VirtualWorker id:alice18 #objects:24>,\n",
              " <VirtualWorker id:alice19 #objects:24>,\n",
              " <VirtualWorker id:alice20 #objects:24>,\n",
              " <VirtualWorker id:alice21 #objects:26>,\n",
              " <VirtualWorker id:alice22 #objects:14>,\n",
              " <VirtualWorker id:alice23 #objects:14>,\n",
              " <VirtualWorker id:alice24 #objects:14>,\n",
              " <VirtualWorker id:alice25 #objects:14>,\n",
              " <VirtualWorker id:alice26 #objects:14>,\n",
              " <VirtualWorker id:alice27 #objects:14>,\n",
              " <VirtualWorker id:alice28 #objects:14>,\n",
              " <VirtualWorker id:alice29 #objects:14>,\n",
              " <VirtualWorker id:alice30 #objects:14>,\n",
              " <VirtualWorker id:alice31 #objects:14>,\n",
              " <VirtualWorker id:alice32 #objects:14>,\n",
              " <VirtualWorker id:alice33 #objects:14>,\n",
              " <VirtualWorker id:alice34 #objects:14>,\n",
              " <VirtualWorker id:alice35 #objects:14>,\n",
              " <VirtualWorker id:alice36 #objects:14>,\n",
              " <VirtualWorker id:alice37 #objects:14>,\n",
              " <VirtualWorker id:alice38 #objects:14>,\n",
              " <VirtualWorker id:alice39 #objects:14>,\n",
              " <VirtualWorker id:alice40 #objects:14>,\n",
              " <VirtualWorker id:alice41 #objects:14>,\n",
              " <VirtualWorker id:alice42 #objects:14>,\n",
              " <VirtualWorker id:alice43 #objects:14>,\n",
              " <VirtualWorker id:alice44 #objects:14>,\n",
              " <VirtualWorker id:alice45 #objects:14>,\n",
              " <VirtualWorker id:alice46 #objects:14>,\n",
              " <VirtualWorker id:alice47 #objects:14>,\n",
              " <VirtualWorker id:alice48 #objects:14>,\n",
              " <VirtualWorker id:alice49 #objects:14>,\n",
              " <VirtualWorker id:alice50 #objects:14>,\n",
              " <VirtualWorker id:alice51 #objects:14>)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH5RrhUaAHwT"
      },
      "source": [
        "# Send Model Segments to starting locations\n",
        "model_locations = alices\n",
        "\n",
        "for model, location in zip(models, model_locations):\n",
        "    model.send(location)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAsG6JP6AHwT"
      },
      "source": [
        "splitNN =  SplitNN(models, optimizers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ykQBcUBAHwT"
      },
      "source": [
        "def train(x, target, splitNN):\n",
        "    \n",
        "    #1) Zero our grads\n",
        "    splitNN.zero_grads()\n",
        "    \n",
        "    #2) Make a prediction\n",
        "    pred = splitNN.forward(x)\n",
        "    # print(pred)\n",
        "    #3) Figure out how much we missed by\n",
        "    criterion = nn.NLLLoss()\n",
        "    loss = criterion(pred, target)\n",
        "  \n",
        "    #4) Backprop the loss on the end layer\n",
        "    loss.backward()\n",
        "    \n",
        "    #5) Feed Gradients backward through the network\n",
        "    splitNN.backward()\n",
        "    \n",
        "    #6) Change the weights\n",
        "    splitNN.step()\n",
        "    \n",
        "    return loss,pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5_QrQA_AHwU"
      },
      "source": [
        "def test(x, target, splitNN):\n",
        "    \n",
        "    pred = splitNN.forward(x)\n",
        "    \n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XGFnMsjFAHwU",
        "outputId": "78efcb6a-0bb2-4b6c-b01d-d23f4d10ff63"
      },
      "source": [
        "models[-1].location.id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'alice51'"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCDS8_IBAHwU"
      },
      "source": [
        "for i in range(epochs):\n",
        "    running_loss = 0\n",
        "    acc = 0\n",
        "    for images, labels in trainloader:\n",
        "        loss = 0\n",
        "        acc_c = 0\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        loss,pred = train(images, labels, splitNN)\n",
        "        # print(pred.shape)\n",
        "        # print(labels.shape)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        acc_c = train_acc/len(labels)\n",
        "        # print(train_acc.get()) \n",
        "        running_loss += loss.get()\n",
        "        acc +=acc_c\n",
        "\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {} Training Accuracy: {}\".format(i, running_loss/len(trainloader),acc/len(trainloader)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR4BLMvpAHwU"
      },
      "source": [
        "testset = datasets.CIFAR100('cifar100', download=True, train=False, transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzG7bzarAHwU"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(testset,shuffle=False)\n",
        "\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQAIyatXAHwU"
      },
      "source": [
        "### Testing the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HBo3ZymAHwV"
      },
      "source": [
        "# loss=0\n",
        "acc_c = 0\n",
        "acc = 0\n",
        "with torch.no_grad():\n",
        " for images, labels in testloader:\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        pred = test(images, labels, splitNN)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        # acc_c = train_acc/len(labels)\n",
        "        # print(images.shape,labels.shape)\n",
        "        acc +=train_acc\n",
        "        # running_loss += loss.get()\n",
        "print(\"Testing accuracy: {}\".format(acc/len(testloader)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvoKrEoQAHwV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5J_WsVBAHwV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4sxEYNmipHR"
      },
      "source": [
        "# 100 Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIRaXYNiipHR"
      },
      "source": [
        "\n",
        "epochs = 2\n",
        "\n",
        "# Define our model segments\n",
        "models = []\n",
        "input_size = 3072\n",
        "hidden_sizes = [128, 640]\n",
        "output_size = 100\n",
        "inp = input_size\n",
        "out = hidden_sizes[0]\n",
        "k=0\n",
        "for i in range(100):\n",
        "\n",
        "                models.append(nn.Sequential(\n",
        "                            nn.Linear(inp, out),\n",
        "                            nn.ReLU(),\n",
        "                ))\n",
        "                inp = hidden_sizes[k]\n",
        "                if k==0:\n",
        "                    k=1\n",
        "                else:\n",
        "                    k=0\n",
        "                out = hidden_sizes[k]\n",
        "models.append(nn.Sequential(\n",
        "                nn.Linear(inp, output_size),\n",
        "                nn.LogSoftmax(dim=1)\n",
        "    ))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlkCHxmgipHR",
        "outputId": "a772810e-1faa-4d21-8a0b-27f0e40a06fd"
      },
      "source": [
        "models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sequential(\n",
              "   (0): Linear(in_features=3072, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=100, bias=True)\n",
              "   (1): LogSoftmax()\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1HDbndvipHS"
      },
      "source": [
        "# Create optimisers for each segment and link to their segment\n",
        "optimizers = [\n",
        "    optim.SGD(model.parameters(), lr=0.03,)\n",
        "    for model in models\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL3IOZ4VipHS"
      },
      "source": [
        "\n",
        "# create some workers\n",
        "alices = []\n",
        "for i in range(101):\n",
        "    alices.append(sy.VirtualWorker(hook, id=\"alice\"+str(i+1)))\n",
        "\n",
        "workers = tuple(alices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1Gmi2gZipHS",
        "outputId": "7c1298cb-1605-4038-ae42-99e6871f3a39"
      },
      "source": [
        "workers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<VirtualWorker id:alice1 #objects:6>,\n",
              " <VirtualWorker id:alice2 #objects:6>,\n",
              " <VirtualWorker id:alice3 #objects:6>,\n",
              " <VirtualWorker id:alice4 #objects:6>,\n",
              " <VirtualWorker id:alice5 #objects:6>,\n",
              " <VirtualWorker id:alice6 #objects:6>,\n",
              " <VirtualWorker id:alice7 #objects:6>,\n",
              " <VirtualWorker id:alice8 #objects:6>,\n",
              " <VirtualWorker id:alice9 #objects:6>,\n",
              " <VirtualWorker id:alice10 #objects:6>,\n",
              " <VirtualWorker id:alice11 #objects:6>,\n",
              " <VirtualWorker id:alice12 #objects:6>,\n",
              " <VirtualWorker id:alice13 #objects:6>,\n",
              " <VirtualWorker id:alice14 #objects:6>,\n",
              " <VirtualWorker id:alice15 #objects:6>,\n",
              " <VirtualWorker id:alice16 #objects:6>,\n",
              " <VirtualWorker id:alice17 #objects:6>,\n",
              " <VirtualWorker id:alice18 #objects:6>,\n",
              " <VirtualWorker id:alice19 #objects:6>,\n",
              " <VirtualWorker id:alice20 #objects:6>,\n",
              " <VirtualWorker id:alice21 #objects:6>,\n",
              " <VirtualWorker id:alice22 #objects:6>,\n",
              " <VirtualWorker id:alice23 #objects:6>,\n",
              " <VirtualWorker id:alice24 #objects:6>,\n",
              " <VirtualWorker id:alice25 #objects:6>,\n",
              " <VirtualWorker id:alice26 #objects:6>,\n",
              " <VirtualWorker id:alice27 #objects:6>,\n",
              " <VirtualWorker id:alice28 #objects:6>,\n",
              " <VirtualWorker id:alice29 #objects:6>,\n",
              " <VirtualWorker id:alice30 #objects:6>,\n",
              " <VirtualWorker id:alice31 #objects:6>,\n",
              " <VirtualWorker id:alice32 #objects:6>,\n",
              " <VirtualWorker id:alice33 #objects:6>,\n",
              " <VirtualWorker id:alice34 #objects:6>,\n",
              " <VirtualWorker id:alice35 #objects:6>,\n",
              " <VirtualWorker id:alice36 #objects:6>,\n",
              " <VirtualWorker id:alice37 #objects:6>,\n",
              " <VirtualWorker id:alice38 #objects:6>,\n",
              " <VirtualWorker id:alice39 #objects:6>,\n",
              " <VirtualWorker id:alice40 #objects:6>,\n",
              " <VirtualWorker id:alice41 #objects:6>,\n",
              " <VirtualWorker id:alice42 #objects:6>,\n",
              " <VirtualWorker id:alice43 #objects:6>,\n",
              " <VirtualWorker id:alice44 #objects:6>,\n",
              " <VirtualWorker id:alice45 #objects:6>,\n",
              " <VirtualWorker id:alice46 #objects:6>,\n",
              " <VirtualWorker id:alice47 #objects:6>,\n",
              " <VirtualWorker id:alice48 #objects:6>,\n",
              " <VirtualWorker id:alice49 #objects:6>,\n",
              " <VirtualWorker id:alice50 #objects:6>,\n",
              " <VirtualWorker id:alice51 #objects:6>,\n",
              " <VirtualWorker id:alice52 #objects:6>,\n",
              " <VirtualWorker id:alice53 #objects:6>,\n",
              " <VirtualWorker id:alice54 #objects:6>,\n",
              " <VirtualWorker id:alice55 #objects:6>,\n",
              " <VirtualWorker id:alice56 #objects:6>,\n",
              " <VirtualWorker id:alice57 #objects:6>,\n",
              " <VirtualWorker id:alice58 #objects:6>,\n",
              " <VirtualWorker id:alice59 #objects:6>,\n",
              " <VirtualWorker id:alice60 #objects:6>,\n",
              " <VirtualWorker id:alice61 #objects:6>,\n",
              " <VirtualWorker id:alice62 #objects:6>,\n",
              " <VirtualWorker id:alice63 #objects:6>,\n",
              " <VirtualWorker id:alice64 #objects:6>,\n",
              " <VirtualWorker id:alice65 #objects:6>,\n",
              " <VirtualWorker id:alice66 #objects:6>,\n",
              " <VirtualWorker id:alice67 #objects:6>,\n",
              " <VirtualWorker id:alice68 #objects:6>,\n",
              " <VirtualWorker id:alice69 #objects:6>,\n",
              " <VirtualWorker id:alice70 #objects:6>,\n",
              " <VirtualWorker id:alice71 #objects:6>,\n",
              " <VirtualWorker id:alice72 #objects:6>,\n",
              " <VirtualWorker id:alice73 #objects:6>,\n",
              " <VirtualWorker id:alice74 #objects:6>,\n",
              " <VirtualWorker id:alice75 #objects:6>,\n",
              " <VirtualWorker id:alice76 #objects:6>,\n",
              " <VirtualWorker id:alice77 #objects:6>,\n",
              " <VirtualWorker id:alice78 #objects:6>,\n",
              " <VirtualWorker id:alice79 #objects:6>,\n",
              " <VirtualWorker id:alice80 #objects:6>,\n",
              " <VirtualWorker id:alice81 #objects:6>,\n",
              " <VirtualWorker id:alice82 #objects:6>,\n",
              " <VirtualWorker id:alice83 #objects:6>,\n",
              " <VirtualWorker id:alice84 #objects:6>,\n",
              " <VirtualWorker id:alice85 #objects:6>,\n",
              " <VirtualWorker id:alice86 #objects:6>,\n",
              " <VirtualWorker id:alice87 #objects:6>,\n",
              " <VirtualWorker id:alice88 #objects:6>,\n",
              " <VirtualWorker id:alice89 #objects:6>,\n",
              " <VirtualWorker id:alice90 #objects:6>,\n",
              " <VirtualWorker id:alice91 #objects:6>,\n",
              " <VirtualWorker id:alice92 #objects:6>,\n",
              " <VirtualWorker id:alice93 #objects:6>,\n",
              " <VirtualWorker id:alice94 #objects:6>,\n",
              " <VirtualWorker id:alice95 #objects:6>,\n",
              " <VirtualWorker id:alice96 #objects:6>,\n",
              " <VirtualWorker id:alice97 #objects:6>,\n",
              " <VirtualWorker id:alice98 #objects:6>,\n",
              " <VirtualWorker id:alice99 #objects:6>,\n",
              " <VirtualWorker id:alice100 #objects:6>,\n",
              " <VirtualWorker id:alice101 #objects:6>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mYwCggZipHS"
      },
      "source": [
        "# Send Model Segments to starting locations\n",
        "model_locations = alices\n",
        "\n",
        "for model, location in zip(models, model_locations):\n",
        "    model.send(location)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQdnRqxXipHS"
      },
      "source": [
        "splitNN =  SplitNN(models, optimizers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uvgLZJCipHS"
      },
      "source": [
        "def train(x, target, splitNN):\n",
        "    \n",
        "    #1) Zero our grads\n",
        "    splitNN.zero_grads()\n",
        "    \n",
        "    #2) Make a prediction\n",
        "    pred = splitNN.forward(x)\n",
        "    # print(pred)\n",
        "    #3) Figure out how much we missed by\n",
        "    criterion = nn.NLLLoss()\n",
        "    loss = criterion(pred, target)\n",
        "  \n",
        "    #4) Backprop the loss on the end layer\n",
        "    loss.backward()\n",
        "    \n",
        "    #5) Feed Gradients backward through the network\n",
        "    splitNN.backward()\n",
        "    \n",
        "    #6) Change the weights\n",
        "    splitNN.step()\n",
        "    \n",
        "    return loss,pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQkcz7VhipHS"
      },
      "source": [
        "def test(x, target, splitNN):\n",
        "    \n",
        "    pred = splitNN.forward(x)\n",
        "    \n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CjXkQHhDipHS",
        "outputId": "a097414d-b091-4fb6-e1e7-f34f1a6b8c99"
      },
      "source": [
        "models[-1].location.id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'alice101'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syIeNX02ipHS"
      },
      "source": [
        "for i in range(epochs):\n",
        "    running_loss = 0\n",
        "    acc = 0\n",
        "    for images, labels in trainloader:\n",
        "        loss = 0\n",
        "        acc_c = 0\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        loss,pred = train(images, labels, splitNN)\n",
        "        # print(pred.shape)\n",
        "        # print(labels.shape)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        acc_c = train_acc/len(labels)\n",
        "        # print(train_acc.get()) \n",
        "        running_loss += loss.get()\n",
        "        acc +=acc_c\n",
        "\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {} Training Accuracy: {}\".format(i, running_loss/len(trainloader),acc/len(trainloader)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8oj1hixipHT",
        "outputId": "aec6c3e0-56c0-4152-f071-0db0b5aa21cf"
      },
      "source": [
        "testset = datasets.CIFAR100('cifar100', download=True, train=False, transform=transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0tK8zu_ipHT",
        "outputId": "985e84f5-26c8-423d-99e0-065088e522c2"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(testset,shuffle=False)\n",
        "\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f865b46e9f0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWMeGQFaipHT"
      },
      "source": [
        "### Testing the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl5hld3mipHT",
        "outputId": "5339fcf7-88ba-4ae5-8e23-b1d240d9396a"
      },
      "source": [
        "# loss=0\n",
        "acc_c = 0\n",
        "acc = 0\n",
        "with torch.no_grad():\n",
        " for images, labels in testloader:\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        pred = test(images, labels, splitNN)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        # acc_c = train_acc/len(labels)\n",
        "        # print(images.shape,labels.shape)\n",
        "        acc +=train_acc\n",
        "        # running_loss += loss.get()\n",
        "print(\"Testing accuracy: {}\".format(acc/len(testloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4FpaOMnipHU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uunM5M8jipHU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}