{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differential_Privacy_varchalav2_final.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varchala/Secure_Private_AI/blob/main/Differential_Privacy_varchalav2_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIWPtp5O1BA5"
      },
      "source": [
        "# !pip install tensorflow-privacy --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXLZ7zywsYvg"
      },
      "source": [
        "!pip install keras==2.2.3 tensorflow_privacy==0.2.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2DcXWzu4Qv4"
      },
      "source": [
        "\"\"\"\n",
        "1. You should train differentially private models on the CIFAR-100 dataset. \n",
        "2. You will use TensorFlow Privacy which provide DP implementations of standard optimizers, but you only need to pick one. \n",
        "3. The goal is to train a well performing model while keeping a small value for epsilon during training. \n",
        "4. You will use a fixed delta of 4 × 10−5. \n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import logging\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import GradientDescentOptimizer\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW_zuhOabtjZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOBQELw7mo9Z"
      },
      "source": [
        "from absl import logging\n",
        "import collections\n",
        "\n",
        "from tensorflow_privacy.privacy.analysis import privacy_ledger\n",
        "from tensorflow_privacy.privacy.dp_query import gaussian_query\n",
        "\n",
        "def make_optimizer_class(cls):\n",
        "  \"\"\"Constructs a DP optimizer class from an existing one.\"\"\"\n",
        "  parent_code = tf.compat.v1.train.Optimizer.compute_gradients.__code__\n",
        "  child_code = cls.compute_gradients.__code__\n",
        "  GATE_OP = tf.compat.v1.train.Optimizer.GATE_OP  # pylint: disable=invalid-name\n",
        "  if child_code is not parent_code:\n",
        "    logging.warning(\n",
        "        'WARNING: Calling make_optimizer_class() on class %s that overrides '\n",
        "        'method compute_gradients(). Check to ensure that '\n",
        "        'make_optimizer_class() does not interfere with overridden version.',\n",
        "        cls.__name__)\n",
        "\n",
        "  class DPOptimizerClass(cls):\n",
        "    \"\"\"Differentially private subclass of given class cls.\"\"\"\n",
        "\n",
        "    _GlobalState = collections.namedtuple(\n",
        "      '_GlobalState', ['l2_norm_clip', 'stddev'])\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        dp_sum_query,\n",
        "        num_microbatches=None,\n",
        "        unroll_microbatches=False,\n",
        "        *args,  # pylint: disable=keyword-arg-before-vararg, g-doc-args\n",
        "        **kwargs):\n",
        "      \"\"\"Initialize the DPOptimizerClass.\n",
        "\n",
        "      Args:\n",
        "        dp_sum_query: DPQuery object, specifying differential privacy\n",
        "          mechanism to use.\n",
        "        num_microbatches: How many microbatches into which the minibatch is\n",
        "          split. If None, will default to the size of the minibatch, and\n",
        "          per-example gradients will be computed.\n",
        "        unroll_microbatches: If true, processes microbatches within a Python\n",
        "          loop instead of a tf.while_loop. Can be used if using a tf.while_loop\n",
        "          raises an exception.\n",
        "      \"\"\"\n",
        "      super(DPOptimizerClass, self).__init__(*args, **kwargs)\n",
        "      self._dp_sum_query = dp_sum_query\n",
        "      self._num_microbatches = num_microbatches\n",
        "      self._global_state = self._dp_sum_query.initial_global_state()\n",
        "      # TODO(b/122613513): Set unroll_microbatches=True to avoid this bug.\n",
        "      # Beware: When num_microbatches is large (>100), enabling this parameter\n",
        "      # may cause an OOM error.\n",
        "      self._unroll_microbatches = unroll_microbatches\n",
        "\n",
        "    def compute_gradients(self,\n",
        "                          loss,\n",
        "                          var_list,\n",
        "                          gate_gradients=GATE_OP,\n",
        "                          aggregation_method=None,\n",
        "                          colocate_gradients_with_ops=False,\n",
        "                          grad_loss=None,\n",
        "                          gradient_tape=None,\n",
        "                          curr_noise_mult=0,\n",
        "                          curr_norm_clip=1):\n",
        "\n",
        "      self._dp_sum_query = gaussian_query.GaussianSumQuery(curr_norm_clip, \n",
        "                                                           curr_norm_clip*curr_noise_mult)\n",
        "      self._global_state = self._dp_sum_query.make_global_state(curr_norm_clip, \n",
        "                                                                curr_norm_clip*curr_noise_mult)\n",
        "      \n",
        "\n",
        "      # TF is running in Eager mode, check we received a vanilla tape.\n",
        "      if not gradient_tape:\n",
        "        raise ValueError('When in Eager mode, a tape needs to be passed.')\n",
        "\n",
        "      vector_loss = loss()\n",
        "      if self._num_microbatches is None:\n",
        "        self._num_microbatches = tf.shape(input=vector_loss)[0]\n",
        "      sample_state = self._dp_sum_query.initial_sample_state(var_list)\n",
        "      microbatches_losses = tf.reshape(vector_loss, [self._num_microbatches, -1])\n",
        "      sample_params = (self._dp_sum_query.derive_sample_params(self._global_state))\n",
        "\n",
        "      def process_microbatch(i, sample_state):\n",
        "        \"\"\"Process one microbatch (record) with privacy helper.\"\"\"\n",
        "        microbatch_loss = tf.reduce_mean(input_tensor=tf.gather(microbatches_losses, [i]))\n",
        "        grads = gradient_tape.gradient(microbatch_loss, var_list)\n",
        "        sample_state = self._dp_sum_query.accumulate_record(sample_params, sample_state, grads)\n",
        "        return sample_state\n",
        "    \n",
        "      for idx in range(self._num_microbatches):\n",
        "        sample_state = process_microbatch(idx, sample_state)\n",
        "\n",
        "      if curr_noise_mult > 0:\n",
        "        grad_sums, self._global_state = (self._dp_sum_query.get_noised_result(sample_state, self._global_state))\n",
        "      else:\n",
        "        grad_sums = sample_state\n",
        "\n",
        "      def normalize(v):\n",
        "        return v / tf.cast(self._num_microbatches, tf.float32)\n",
        "\n",
        "      final_grads = tf.nest.map_structure(normalize, grad_sums)\n",
        "      grads_and_vars = final_grads#list(zip(final_grads, var_list))\n",
        "    \n",
        "      return grads_and_vars\n",
        "\n",
        "  return DPOptimizerClass\n",
        "\n",
        "\n",
        "def make_gaussian_optimizer_class(cls):\n",
        "  \"\"\"Constructs a DP optimizer with Gaussian averaging of updates.\"\"\"\n",
        "\n",
        "  class DPGaussianOptimizerClass(make_optimizer_class(cls)):\n",
        "    \"\"\"DP subclass of given class cls using Gaussian averaging.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        l2_norm_clip,\n",
        "        noise_multiplier,\n",
        "        num_microbatches=None,\n",
        "        ledger=None,\n",
        "        unroll_microbatches=False,\n",
        "        *args,  # pylint: disable=keyword-arg-before-vararg\n",
        "        **kwargs):\n",
        "      dp_sum_query = gaussian_query.GaussianSumQuery(\n",
        "          l2_norm_clip, l2_norm_clip * noise_multiplier)\n",
        "\n",
        "      if ledger:\n",
        "        dp_sum_query = privacy_ledger.QueryWithLedger(dp_sum_query,\n",
        "                                                      ledger=ledger)\n",
        "\n",
        "      super(DPGaussianOptimizerClass, self).__init__(\n",
        "          dp_sum_query,\n",
        "          num_microbatches,\n",
        "          unroll_microbatches,\n",
        "          *args,\n",
        "          **kwargs)\n",
        "\n",
        "    @property\n",
        "    def ledger(self):\n",
        "      return self._dp_sum_query.ledger\n",
        "\n",
        "  return DPGaussianOptimizerClass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N3P2HX9se3-"
      },
      "source": [
        "GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer\n",
        "DPGradientDescentGaussianOptimizer_NEW = make_gaussian_optimizer_class(GradientDescentOptimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g-MzYgYsytA"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "discriminator_optimizer = DPGradientDescentGaussianOptimizer_NEW(\n",
        "   learning_rate = 0.01,\n",
        "   l2_norm_clip = 1.5,\n",
        "   noise_multiplier = 1.3,\n",
        "   num_microbatches = 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUE8XTz8QXY_"
      },
      "source": [
        "# optimizer = GradientDescentOptimizer(1e-4)\n",
        "\n",
        "# optimizer = DPGradientDescentGaussianOptimizer(\n",
        "#     l2_norm_clip=1.5,\n",
        "#     noise_multiplier=1.3,\n",
        "#     num_microbatches=250,\n",
        "#     learning_rate=0.25)\n",
        "# loss = tf.keras.losses.CategoricalCrossentropy( from_logits=True, reduction=tf.losses.Reduction.NONE )\n",
        "# # train_op = optimizer.minimize(loss=scalar_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB6et2zT5orb"
      },
      "source": [
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar100.load_data()\n",
        "\n",
        "\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# split data into validation and training set\n",
        "validation_images = train_images[:5000]\n",
        "validation_labels = train_labels[:5000]\n",
        "train_images = train_images[5000:]\n",
        "train_labels = train_labels[5000:]\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(100, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer=discriminator_optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "data_generator = ImageDataGenerator(width_shift_range=0.1,\n",
        "                                    height_shift_range=0.1,\n",
        "                                    horizontal_flip=True)\n",
        "\n",
        "\n",
        "train_data_iterator = data_generator.flow(train_images, train_labels, batch_size=64)\n",
        "\n",
        "\n",
        "test_data_iterator = data_generator.flow(test_images, test_labels, batch_size=64)\n",
        "\n",
        "\n",
        "validation_data_iterator = data_generator.flow(validation_images, validation_labels, batch_size=64)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q3KbsycQtux"
      },
      "source": [
        "\n",
        "epsilon, delta = compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=60000, batch_size=100, noise_multiplier=1.2, epochs=15, delta=4e-5)\n",
        "\n",
        "\n",
        "print(\"Epsilon: \", epsilon)\n",
        "\n",
        "# train model\n",
        "model.fit(train_data_iterator,\n",
        "          epochs=5,\n",
        "          validation_data=validation_data_iterator,\n",
        "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])\n",
        "\n",
        "# evaluate model\n",
        "test_loss, test_acc = model.evaluate(test_data_iterator, verbose=2)\n",
        "\n",
        "\n",
        "train_loss, train_acc = model.evaluate(train_data_iterator, verbose=2)\n",
        "\n",
        "\n",
        "\n",
        "print('\\n Test accuracy:', test_acc)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}